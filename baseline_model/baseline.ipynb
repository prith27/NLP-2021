{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<p>It is advisable that you read our introductory documentation webpage before moving on with understading the code. As it would help you understand the problem better.</p>\n",
    "<p>You can check it out <a href=\"https://hasocfire.github.io/hasoc/2021/ichcl/index.html\">here</a></p>"
   ],
   "metadata": {
    "id": "narrative-emphasis"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importing Libraries and initializing stopwords and stemmer"
   ],
   "metadata": {
    "id": "continuing-violation"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import re\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import stemmer as hindi_stemmer"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-08-19 06:08:54.529431: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-08-19 06:08:54.529458: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "metadata": {
    "id": "fossil-enlargement"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "nltk.download('stopwords')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sri/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>Making a list of english and hindi stopwords. <br>The enlgish stopwords are retrieved from NLTK library as well. <br>And the hindi stopwords are retrieved from a data set on Mendeley Data. To read about how the authors compiled the list, you can check their <a href = \"https://arxiv.org/ftp/arxiv/papers/2002/2002.00171.pdf\" > publicaion </a> </p>\n",
    "<p>Initializing an english SnowballStemmer using the NLTK library. <br>And the hindi stemmer used was produced by students of Banasthali University. You can check out their <a href=\"https://arxiv.org/ftp/arxiv/papers/1305/1305.6211.pdf\">publication</a></p>"
   ],
   "metadata": {
    "id": "greenhouse-kitchen"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "english_stopwords = stopwords.words(\"english\")\n",
    "with open('final_stopwords.txt', encoding = 'utf-8') as f:\n",
    "    hindi_stopwords = f.readlines()\n",
    "    for i in range(len(hindi_stopwords)):\n",
    "        hindi_stopwords[i] = re.sub('\\n','',hindi_stopwords[i])\n",
    "stopwords = english_stopwords + hindi_stopwords\n",
    "english_stemmer = SnowballStemmer(\"english\")"
   ],
   "outputs": [],
   "metadata": {
    "id": "gentle-richards"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reading Data"
   ],
   "metadata": {
    "id": "YeWusmHa9aFc"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>Initializing a list of various directories that data is stored in using the glob Library.</p>"
   ],
   "metadata": {
    "id": "appreciated-bristol"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "train_directories = []\n",
    "for i in glob(\"data/train/*/\"):\n",
    "    for j in glob(i+'*/'):\n",
    "        train_directories.append(j)"
   ],
   "outputs": [],
   "metadata": {
    "id": "labeled-saskatchewan"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "train_directories"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['data/train/bantwitter/1397101600460529665/',\n",
       " 'data/train/bantwitter/1397235232621932545/',\n",
       " 'data/train/bantwitter/1397245294597783556/',\n",
       " 'data/train/bantwitter/1397895650218430465/',\n",
       " 'data/train/bantwitter/1397914911431217152/',\n",
       " 'data/train/bantwitter/1397923242938015749/',\n",
       " 'data/train/bantwitter/1397959669696634885/',\n",
       " 'data/train/casteism/1391715849979924484/',\n",
       " 'data/train/casteism/1394701718458310661/',\n",
       " 'data/train/casteism/1394894310781321218/',\n",
       " 'data/train/casteism/1394920395438886917/',\n",
       " 'data/train/casteism/1394929122237845509/',\n",
       " 'data/train/casteism/1395690830292160520/',\n",
       " 'data/train/casteism/1395747405786537988/',\n",
       " 'data/train/casteism/1395768553190604804/',\n",
       " 'data/train/casteism/1397952483532566536/',\n",
       " 'data/train/casteism/1398675045266853888/',\n",
       " 'data/train/charlie hebdo/1392703278022938626/',\n",
       " 'data/train/charlie hebdo/1392704604387770374/',\n",
       " 'data/train/charlie hebdo/1392715113853964288/',\n",
       " 'data/train/charlie hebdo/1392717752570241024/',\n",
       " 'data/train/Covid Crisis/1383261325531353098/',\n",
       " 'data/train/Covid Crisis/1385527614857646083/',\n",
       " 'data/train/Covid Crisis/1385779864649691136/',\n",
       " 'data/train/Covid Crisis/1386182358219460609/',\n",
       " 'data/train/Covid Crisis/1386401947729633280/',\n",
       " 'data/train/Covid Crisis/1390905967655546883/',\n",
       " 'data/train/Covid Crisis/1391494336534093826/',\n",
       " 'data/train/Covid Crisis/1391720958461808641/',\n",
       " 'data/train/Covid Crisis/1391990111223500802/',\n",
       " 'data/train/Covid Crisis/1392742952896659456/',\n",
       " 'data/train/Covid Crisis/1393591100741275653/',\n",
       " 'data/train/Covid Crisis/1394118403229503488/',\n",
       " 'data/train/Covid Crisis/1394528104492658700/',\n",
       " 'data/train/Covid Crisis/1395594496641880069/',\n",
       " 'data/train/Covid Crisis/1395650386111778819/',\n",
       " 'data/train/Covid Crisis/1396136884791767044/',\n",
       " 'data/train/Covid Crisis/1398266366889705482/',\n",
       " 'data/train/Covid Crisis/1398268834998411273/',\n",
       " 'data/train/Covid Crisis/1398605185929277440/',\n",
       " 'data/train/indian politics/1287772170538479627/',\n",
       " 'data/train/indian politics/1390245301579968512/',\n",
       " 'data/train/indian politics/1390378715360612357/',\n",
       " 'data/train/indian politics/1390979182608326658/',\n",
       " 'data/train/indian politics/1393620880840921088/',\n",
       " 'data/train/indian politics/1394390667514712065/',\n",
       " 'data/train/indian politics/1395677189870211073/',\n",
       " 'data/train/indian politics/1395691784244580362/',\n",
       " 'data/train/indian politics/1395707691197026307/',\n",
       " 'data/train/indian politics/1396178670587236352/',\n",
       " 'data/train/indian politics/1398308636653355013/',\n",
       " 'data/train/indian politics/1398598597428482050/',\n",
       " 'data/train/indian politics/1399042598401196036/',\n",
       " 'data/train/indian politics/1399043076153282564/',\n",
       " 'data/train/indian politics/1399045661434298374/',\n",
       " 'data/train/indian politics/1413974963535552512/',\n",
       " 'data/train/indian politics/676627652204060672/',\n",
       " 'data/train/Israel/1391513979369914368/',\n",
       " 'data/train/Israel/1391754900896468996/',\n",
       " 'data/train/Israel/1392156226427916288/',\n",
       " 'data/train/Israel/1392178612246257667/',\n",
       " 'data/train/Israel/1392333796901408771/',\n",
       " 'data/train/Israel/1392377122065289216/',\n",
       " 'data/train/Israel/1393200211598614529/',\n",
       " 'data/train/Israel/1394318424893886464/',\n",
       " 'data/train/religious controversies/1391768816355446787/',\n",
       " 'data/train/religious controversies/1392421466243837954/',\n",
       " 'data/train/religious controversies/1392467674848919552/',\n",
       " 'data/train/religious controversies/1392725164060594178/',\n",
       " 'data/train/religious controversies/1393625939741794306/',\n",
       " 'data/train/religious controversies/1393764797859078144/',\n",
       " 'data/train/religious controversies/1394096960575926275/',\n",
       " 'data/train/religious controversies/1394282990377738243/',\n",
       " 'data/train/religious controversies/1394301726610497538/',\n",
       " 'data/train/religious controversies/1394915940077162496/',\n",
       " 'data/train/religious controversies/1394930678106492931/',\n",
       " 'data/train/religious controversies/1394942092325974020/',\n",
       " 'data/train/wuhan virus/1390863059334406149/',\n",
       " 'data/train/wuhan virus/1390947358813261824/',\n",
       " 'data/train/wuhan virus/1391744037753868289/',\n",
       " 'data/train/wuhan virus/1392450559693651968/',\n",
       " 'data/train/wuhan virus/1397974860232544266/']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {
    "id": "patent-release",
    "outputId": "090fa4f9-245d-4086-aed6-07f864a57faf",
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>Reading tree structured data from the directories from the .json files</p>"
   ],
   "metadata": {
    "id": "voluntary-stocks"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "data = []\n",
    "for i in train_directories:\n",
    "    with open(i+'data.json', encoding='utf-8') as f:\n",
    "        data.append(json.load(f))\n",
    "labels = []\n",
    "for i in train_directories:\n",
    "    with open(i+'labels.json', encoding='utf-8') as f:\n",
    "        labels.append(json.load(f))"
   ],
   "outputs": [],
   "metadata": {
    "id": "architectural-chase"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "</p>Defining 2 functions that will turn the data from a tree structure to a flat structure.</p>\n",
    "<ul>\n",
    "    <li>tr_flatten: This is to flat the train data. It takes two variables as function parameters. First one is the tweet data and second one is labels. It'll create a list of json structures like following:\n",
    "        <ul>\n",
    "            <li> for source tweet: It'll create json with tweet_id, tweet text and label. </li>\n",
    "            <li> for comment: It'll create json with tweet_id, label and for the text part it'll append the comment after the source tweet. This is a basic technique to provide context of source tweet. </li>\n",
    "            <li> for reply: It'll create json with tweet_id, label and for the text part it'll append the reply after the comment after the source tweet. So the text here will look like \"source tweet-comment-reply\"</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>te_flatten: This is to flat the test data. It works similarly like tr_flatten but without the labels file, as labels won't be available for test set. It'll be used once the test data is available</li>\n",
    "</ul>"
   ],
   "metadata": {
    "id": "threatened-algorithm"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def tr_flatten(d,l):\n",
    "    flat_text = []\n",
    "    flat_text.append({\n",
    "        'tweet_id':d['tweet_id'],\n",
    "        'text':d['tweet'],\n",
    "        'label':l[d['tweet_id']]\n",
    "    })\n",
    "\n",
    "    for i in d['comments']:\n",
    "            flat_text.append({\n",
    "                'tweet_id':i['tweet_id'],\n",
    "                'text':flat_text[0]['text'] +' '+i['tweet'], #flattening comments(appending one after the other)\n",
    "                'label':l[i['tweet_id']]\n",
    "            })\n",
    "            if 'replies' in i.keys():\n",
    "                for j in i['replies']:\n",
    "                    flat_text.append({\n",
    "                        'tweet_id':j['tweet_id'],\n",
    "                        'text':flat_text[0]['text'] +' '+ i['tweet'] +' '+ j['tweet'], #flattening replies\n",
    "                        'label':l[j['tweet_id']]\n",
    "                    })\n",
    "    return flat_text\n",
    "\n",
    "def te_flatten(d):\n",
    "    flat_text = []\n",
    "    flat_text.append({\n",
    "        'tweet_id':d['tweet_id'],\n",
    "        'text':d['tweet'],\n",
    "    })\n",
    "\n",
    "    for i in d['comments']:\n",
    "            flat_text.append({\n",
    "                'tweet_id':i['tweet_id'],\n",
    "                'text':flat_text[0]['text'] + i['tweet'],\n",
    "            })\n",
    "            if 'replies' in i.keys():\n",
    "                for j in i['replies']:\n",
    "                    flat_text.append({\n",
    "                        'tweet_id':j['tweet_id'],\n",
    "                        'text':flat_text[0]['text'] + i['tweet'] + j['tweet'],\n",
    "                    })\n",
    "    return flat_text"
   ],
   "outputs": [],
   "metadata": {
    "id": "wired-anniversary"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>This cell will run both the flatten functions. Again, you can skip the test part if it is not available. The train_len variable will be used later on for splitting the data.</p>"
   ],
   "metadata": {
    "id": "cardiac-catholic"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "data_label = []\n",
    "#for train\n",
    "for i in range(len(labels)):\n",
    "    for j in tr_flatten(data[i], labels[i]):\n",
    "        data_label.append(j)\n",
    "train_len = len(data_label)"
   ],
   "outputs": [],
   "metadata": {
    "id": "wrapped-cancellation"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "df = pd.DataFrame(data_label, columns = data_label[0].keys(), index = None)"
   ],
   "outputs": [],
   "metadata": {
    "id": "limiting-lighter"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              tweet_id                                               text  \\\n",
       "0  1397101600460529665  Countries which have Banned Twitter\\n\\n🇨🇳 Chin...   \n",
       "1  1397101827116703744  Countries which have Banned Twitter\\n\\n🇨🇳 Chin...   \n",
       "2  1397101939674869763  Countries which have Banned Twitter\\n\\n🇨🇳 Chin...   \n",
       "3  1397102700173488133  Countries which have Banned Twitter\\n\\n🇨🇳 Chin...   \n",
       "4  1397102906004754433  Countries which have Banned Twitter\\n\\n🇨🇳 Chin...   \n",
       "\n",
       "  label  \n",
       "0   HOF  \n",
       "1  NONE  \n",
       "2   HOF  \n",
       "3   HOF  \n",
       "4   HOF  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1397101600460529665</td>\n",
       "      <td>Countries which have Banned Twitter\\n\\n🇨🇳 Chin...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1397101827116703744</td>\n",
       "      <td>Countries which have Banned Twitter\\n\\n🇨🇳 Chin...</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1397101939674869763</td>\n",
       "      <td>Countries which have Banned Twitter\\n\\n🇨🇳 Chin...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1397102700173488133</td>\n",
       "      <td>Countries which have Banned Twitter\\n\\n🇨🇳 Chin...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1397102906004754433</td>\n",
       "      <td>Countries which have Banned Twitter\\n\\n🇨🇳 Chin...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {
    "id": "functioning-asthma",
    "outputId": "5f577c5d-2df1-47ff-c300-6f65b7aa4c45"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "df['label'].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NONE    2899\n",
       "HOF     2841\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {
    "id": "searching-batman",
    "outputId": "82708ad2-d18d-4f3b-9a01-33d8beb75b16"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "tweets = df.text\n",
    "y = df.label"
   ],
   "outputs": [],
   "metadata": {
    "id": "treated-machinery"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing and featuring the raw text"
   ],
   "metadata": {
    "id": "COEGvpuj9hZ5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>This is a preprocessing function and the regex will match with anything that is not English, Hindi and Emoji.</p>\n",
    "<p>The preprocessing steps are as followed:</p>\n",
    "<ul>\n",
    "    <li>Remove Handles</li>\n",
    "    <li>Remove URLs</li>    \n",
    "    <li>Remove anything that is not English, Hindi and Emoji</li>    \n",
    "    <li>Remove RT which appears in retweets</li>    \n",
    "    <li>Remove Abundant Newlines</li>    \n",
    "    <li>Remove Abundant whitespaces</li>    \n",
    "    <li>Remove Stopwords</li>\n",
    "    <li>Stem English text</li>\n",
    "    <li>Stem Hindi text</li>\n",
    "</ul>"
   ],
   "metadata": {
    "id": "conscious-dress"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "regex_for_english_hindi_emojis=\"[^a-zA-Z#\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF\\u0900-\\u097F]\"\n",
    "def clean_tweet(tweet):\n",
    "    tweet = re.sub(r\"@[A-Za-z0-9]+\",' ', tweet)\n",
    "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\",' ', tweet)\n",
    "    tweet = re.sub(regex_for_english_hindi_emojis,' ', tweet)\n",
    "    tweet = re.sub(\"RT \", \" \", tweet)\n",
    "    tweet = re.sub(\"\\n\", \" \", tweet)\n",
    "    tweet = re.sub(r\" +\", \" \", tweet)\n",
    "    tokens = []\n",
    "    for token in tweet.split():\n",
    "        if token not in stopwords:\n",
    "            token = english_stemmer.stem(token)\n",
    "            token = hindi_stemmer.hi_stem(token)\n",
    "            tokens.append(token)\n",
    "    return \" \".join(tokens)"
   ],
   "outputs": [],
   "metadata": {
    "id": "included-bearing"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "cleaned_tweets = [clean_tweet(tweet) for tweet in tweets]"
   ],
   "outputs": [],
   "metadata": {
    "id": "decimal-sarah"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>Using TF-IDF for featuring the text. The vectorizer will only consider vocab terms that appear in more than 5 documents.</p>\n",
    "<p>To learn more about TF-IDF you can check <a href = \"https://towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558\">here</a> and <a href = \"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">here</a>.</p>"
   ],
   "metadata": {
    "id": "august-gateway"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 5)\n",
    "X = vectorizer.fit_transform(cleaned_tweets)\n",
    "X = X.todense()"
   ],
   "outputs": [],
   "metadata": {
    "id": "floating-hudson"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training and evaluating model"
   ],
   "metadata": {
    "id": "wJ8kmLB29mbC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "metadata": {
    "id": "periodic-intake"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>Training the Logistic Regression classifier provided by Scikit-Learn library.</p>\n",
    "<p>To learn more about Logistic Regression classifier you can check <a href = \"https://www.youtube.com/watch?v=yIYKR4sgzI8\">here</a> and <a href = \"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\">here</a>.</p>"
   ],
   "metadata": {
    "id": "compatible-airfare"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {
    "id": "emotional-subsection",
    "outputId": "63e90d48-dfe5-411c-8d66-b9a9ea267589"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>Predicting and priting classification metrics for validation set.</p>"
   ],
   "metadata": {
    "id": "connected-soccer"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "y_pred = classifier.predict(X_val)"
   ],
   "outputs": [],
   "metadata": {
    "id": "arabic-appreciation"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "print(classification_report(y_val, y_pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         HOF       0.73      0.70      0.72       577\n",
      "        NONE       0.71      0.74      0.73       571\n",
      "\n",
      "    accuracy                           0.72      1148\n",
      "   macro avg       0.72      0.72      0.72      1148\n",
      "weighted avg       0.72      0.72      0.72      1148\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "id": "challenging-intersection",
    "outputId": "2e8bea3f-60e2-4863-909f-1591f9b2e04a"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "le = LabelEncoder() #label encoding labels for training Dense Neural Network\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_val = le.transform(y_val)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dense(32, activation=\"relu\"),\n",
    "        Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "model.compile('adam', loss='binary_crossentropy', metrics = ['accuracy']) #compiling a neural network with 3 layers for classification"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-08-19 06:09:13.004973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-19 06:09:13.006327: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-08-19 06:09:13.006774: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2021-08-19 06:09:13.007214: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2021-08-19 06:09:13.007655: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2021-08-19 06:09:13.008095: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2021-08-19 06:09:13.008550: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2021-08-19 06:09:13.009078: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2021-08-19 06:09:13.009636: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-08-19 06:09:13.009671: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-08-19 06:09:13.010159: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "model.fit(X_train, y_train, epochs = 5, batch_size = 32)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-08-19 06:09:14.007919: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "144/144 [==============================] - 1s 4ms/step - loss: 0.6346 - accuracy: 0.6544\n",
      "Epoch 2/5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 0.5317 - accuracy: 0.7448\n",
      "Epoch 3/5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 0.4567 - accuracy: 0.7905\n",
      "Epoch 4/5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 0.3935 - accuracy: 0.8297\n",
      "Epoch 5/5\n",
      "144/144 [==============================] - 0s 3ms/step - loss: 0.3407 - accuracy: 0.8591\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fca94052190>"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "y_pred = model.predict(X_val)\n",
    "y_pred = (y_pred > 0.5).astype('int64')\n",
    "y_pred = y_pred.reshape(len(y_pred))    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "print(classification_report(y_val, y_pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.66      0.69       577\n",
      "           1       0.69      0.74      0.71       571\n",
      "\n",
      "    accuracy                           0.70      1148\n",
      "   macro avg       0.71      0.70      0.70      1148\n",
      "weighted avg       0.71      0.70      0.70      1148\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predicting test data and making a sample submission file"
   ],
   "metadata": {
    "id": "satisfactory-affect"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>This part will be used to read and make predictions on the test data once the it is made available. When it is available, make a directory in data directory as 'test' and copy the story direcotries into the test directory.</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "test_directories = []\n",
    "for i in glob(\"data/test/*/\"):\n",
    "    for j in glob(i+'*/'):\n",
    "        test_directories.append(j)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "test_directories"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>The test directories do not contain labels.json file so labels list is not initialized for test data.</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "test_data = []\n",
    "for i in test_directories:\n",
    "    with open(i+'data.json', encoding='utf-8') as f:\n",
    "        data.append(json.load(f))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>Flattening the test data.</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_tweetid_data = []\n",
    "#for test\n",
    "for i in range(len(labels), len(data)):\n",
    "    for j in te_flatten(data[i]):\n",
    "        test_tweetid_data.append(j)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_df = pd.DataFrame(test_tweetid_data, columns = test_tweetid_data[0].keys(), index = None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_tweets = test_df.text\n",
    "tweet_ids = test_df.tweet_id"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cleaned_test = [clean_tweet(tweet) for tweet in test_tweets]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_test = vectorizer.transform(cleaned_test)\n",
    "X_test = X_test.todense()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "submission_prediction = classifier.predict(X_test)\n",
    "submission = {'tweet_id': tweet_ids, 'label':submission_prediction}\n",
    "submission = pd.DataFrame(submission)"
   ],
   "outputs": [],
   "metadata": {
    "id": "crude-fleet"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "submission.to_csv('data/sample_submission.csv', index = False)"
   ],
   "outputs": [],
   "metadata": {
    "id": "hired-relief"
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "continuing-violation",
    "YeWusmHa9aFc",
    "COEGvpuj9hZ5",
    "wJ8kmLB29mbC",
    "satisfactory-affect"
   ],
   "name": "baseline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "interpreter": {
   "hash": "90ba7c39c09c5af6f3bd617723ce73acc5546916eb6ee5fa8d6f823a2e9aaebc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}