{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "895c995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRITHVI SESHADRI\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5056678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a34daf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file=\"shakespeare.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cf77344",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=open(path_to_file,'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11b20a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted are\n",
      "  From his low tract and look another way:\n",
      "    So thou, thy self out-going in thy noon:\n",
      "    Unlooked on diest unless thou get a son.\n",
      "\n",
      "\n",
      "                     8\n",
      "  Music to hear, why hear'st thou music sadly?\n",
      "  Sweets with sweets war not, joy delights in joy:\n",
      "  Why lov'st thou that which t\n"
     ]
    }
   ],
   "source": [
    "print(text[4500:4800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "338a3a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '}']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(vocab)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cafd568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '>',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " ']',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '|',\n",
       " '}']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8c1be44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e686e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ind = {u:i for i, u in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d42d145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '\\n',\n",
       " 1: ' ',\n",
       " 2: '!',\n",
       " 3: '\"',\n",
       " 4: '&',\n",
       " 5: \"'\",\n",
       " 6: '(',\n",
       " 7: ')',\n",
       " 8: ',',\n",
       " 9: '-',\n",
       " 10: '.',\n",
       " 11: '0',\n",
       " 12: '1',\n",
       " 13: '2',\n",
       " 14: '3',\n",
       " 15: '4',\n",
       " 16: '5',\n",
       " 17: '6',\n",
       " 18: '7',\n",
       " 19: '8',\n",
       " 20: '9',\n",
       " 21: ':',\n",
       " 22: ';',\n",
       " 23: '<',\n",
       " 24: '>',\n",
       " 25: '?',\n",
       " 26: 'A',\n",
       " 27: 'B',\n",
       " 28: 'C',\n",
       " 29: 'D',\n",
       " 30: 'E',\n",
       " 31: 'F',\n",
       " 32: 'G',\n",
       " 33: 'H',\n",
       " 34: 'I',\n",
       " 35: 'J',\n",
       " 36: 'K',\n",
       " 37: 'L',\n",
       " 38: 'M',\n",
       " 39: 'N',\n",
       " 40: 'O',\n",
       " 41: 'P',\n",
       " 42: 'Q',\n",
       " 43: 'R',\n",
       " 44: 'S',\n",
       " 45: 'T',\n",
       " 46: 'U',\n",
       " 47: 'V',\n",
       " 48: 'W',\n",
       " 49: 'X',\n",
       " 50: 'Y',\n",
       " 51: 'Z',\n",
       " 52: '[',\n",
       " 53: ']',\n",
       " 54: '_',\n",
       " 55: '`',\n",
       " 56: 'a',\n",
       " 57: 'b',\n",
       " 58: 'c',\n",
       " 59: 'd',\n",
       " 60: 'e',\n",
       " 61: 'f',\n",
       " 62: 'g',\n",
       " 63: 'h',\n",
       " 64: 'i',\n",
       " 65: 'j',\n",
       " 66: 'k',\n",
       " 67: 'l',\n",
       " 68: 'm',\n",
       " 69: 'n',\n",
       " 70: 'o',\n",
       " 71: 'p',\n",
       " 72: 'q',\n",
       " 73: 'r',\n",
       " 74: 's',\n",
       " 75: 't',\n",
       " 76: 'u',\n",
       " 77: 'v',\n",
       " 78: 'w',\n",
       " 79: 'x',\n",
       " 80: 'y',\n",
       " 81: 'z',\n",
       " 82: '|',\n",
       " 83: '}'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a41646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_to_char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f3a9152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', ',', '-', '.', '0', '1',\n",
       "       '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '>', '?',\n",
       "       'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
       "       'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
       "       '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i',\n",
       "       'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v',\n",
       "       'w', 'x', 'y', 'z', '|', '}'], dtype='<U1')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09896c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = np.array([char_to_ind[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "232f5f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  1, ..., 30, 39, 29])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8436dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f99accb",
   "metadata": {},
   "source": [
    "# Creating batch for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8aa35342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bu\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4297c17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "line = \"From fairest creatures we desire increase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f187b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74cb2cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_stanza = \"\"\"From fairest creatures we desire increase,\n",
    "  That thereby beauty's rose might never die,\n",
    "  But as the riper should by time decease,\"\"\"   \n",
    "#Using 3 lines cause in Shakespeare's writing there can be similarities evry 3 lines. Checked google for this fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed0cb8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(part_stanza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21cf7e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3b54323",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_seq = len(text)//(seq_len+1)  #Number of sequences in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b6c5682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45005"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_num_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79c8c63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "1\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "F\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "f\n",
      "a\n",
      "i\n",
      "r\n",
      "e\n",
      "s\n",
      "t\n",
      " \n",
      "c\n",
      "r\n",
      "e\n",
      "a\n",
      "t\n",
      "u\n",
      "r\n",
      "e\n",
      "s\n",
      " \n",
      "w\n",
      "e\n",
      " \n",
      "d\n",
      "e\n",
      "s\n",
      "i\n",
      "r\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "c\n",
      "r\n",
      "e\n",
      "a\n",
      "s\n",
      "e\n",
      ",\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "T\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "r\n",
      "e\n",
      "b\n",
      "y\n",
      " \n",
      "b\n",
      "e\n",
      "a\n",
      "u\n",
      "t\n",
      "y\n",
      "'\n",
      "s\n",
      " \n",
      "r\n",
      "o\n",
      "s\n",
      "e\n",
      " \n",
      "m\n",
      "i\n",
      "g\n",
      "h\n",
      "t\n",
      " \n",
      "n\n",
      "e\n",
      "v\n",
      "e\n",
      "r\n",
      " \n",
      "d\n",
      "i\n",
      "e\n",
      ",\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "B\n",
      "u\n",
      "t\n",
      " \n",
      "a\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "r\n",
      "i\n",
      "p\n",
      "e\n",
      "r\n",
      " \n",
      "s\n",
      "h\n",
      "o\n",
      "u\n",
      "l\n",
      "d\n",
      " \n",
      "b\n",
      "y\n",
      " \n",
      "t\n",
      "i\n",
      "m\n",
      "e\n",
      " \n",
      "d\n",
      "e\n",
      "c\n",
      "e\n",
      "a\n",
      "s\n",
      "e\n",
      ",\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "H\n",
      "i\n",
      "s\n",
      " \n",
      "t\n",
      "e\n",
      "n\n",
      "d\n",
      "e\n",
      "r\n",
      " \n",
      "h\n",
      "e\n",
      "i\n",
      "r\n",
      " \n",
      "m\n",
      "i\n",
      "g\n",
      "h\n",
      "t\n",
      " \n",
      "b\n",
      "e\n",
      "a\n",
      "r\n",
      " \n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "m\n",
      "e\n",
      "m\n",
      "o\n",
      "r\n",
      "y\n",
      ":\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "B\n",
      "u\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "o\n",
      "u\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "t\n",
      "r\n",
      "a\n",
      "c\n",
      "t\n",
      "e\n",
      "d\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "n\n",
      "e\n",
      " \n",
      "o\n",
      "w\n",
      "n\n",
      " \n",
      "b\n",
      "r\n",
      "i\n",
      "g\n",
      "h\n",
      "t\n",
      " \n",
      "e\n",
      "y\n",
      "e\n",
      "s\n",
      ",\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "F\n",
      "e\n",
      "e\n",
      "d\n",
      "'\n",
      "s\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "y\n",
      " \n",
      "l\n",
      "i\n",
      "g\n",
      "h\n",
      "t\n",
      "'\n",
      "s\n",
      " \n",
      "f\n",
      "l\n",
      "a\n",
      "m\n",
      "e\n",
      " \n",
      "w\n",
      "i\n",
      "t\n",
      "h\n",
      " \n",
      "s\n",
      "e\n",
      "l\n",
      "f\n",
      "-\n",
      "s\n",
      "u\n",
      "b\n",
      "s\n",
      "t\n",
      "a\n",
      "n\n",
      "t\n",
      "i\n",
      "a\n",
      "l\n",
      " \n",
      "f\n",
      "u\n",
      "e\n",
      "l\n",
      ",\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "M\n",
      "a\n",
      "k\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "a\n",
      " \n",
      "f\n",
      "a\n",
      "m\n",
      "i\n",
      "n\n",
      "e\n",
      " \n",
      "w\n",
      "h\n",
      "e\n",
      "r\n",
      "e\n",
      " \n",
      "a\n",
      "b\n",
      "u\n",
      "n\n",
      "d\n",
      "a\n",
      "n\n",
      "c\n",
      "e\n",
      " \n",
      "l\n",
      "i\n",
      "e\n",
      "s\n",
      ",\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "T\n",
      "h\n",
      "y\n",
      " \n",
      "s\n",
      "e\n",
      "l\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "y\n",
      " \n",
      "f\n",
      "o\n",
      "e\n",
      ",\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "t\n",
      "h\n",
      "y\n",
      " \n",
      "s\n",
      "w\n",
      "e\n",
      "e\n",
      "t\n",
      " \n",
      "s\n",
      "e\n",
      "l\n",
      "f\n",
      " \n",
      "t\n",
      "o\n",
      "o\n",
      " \n",
      "c\n",
      "r\n",
      "u\n",
      "e\n",
      "l\n",
      ":\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "T\n",
      "h\n",
      "o\n",
      "u\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "a\n",
      "r\n",
      "t\n",
      " \n",
      "n\n",
      "o\n",
      "w\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "l\n",
      "d\n",
      "'\n",
      "s\n",
      " \n",
      "f\n",
      "r\n",
      "e\n",
      "s\n",
      "h\n",
      " \n",
      "o\n",
      "r\n",
      "n\n",
      "a\n",
      "m\n",
      "e\n",
      "n\n",
      "t\n",
      ",\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "A\n",
      "n\n",
      "d\n",
      " \n",
      "o\n",
      "n\n",
      "l\n",
      "y\n",
      " \n",
      "h\n",
      "e\n",
      "r\n",
      "a\n",
      "l\n",
      "d\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "g\n",
      "a\n",
      "u\n",
      "d\n",
      "y\n",
      " \n",
      "s\n",
      "p\n",
      "r\n",
      "i\n",
      "n\n",
      "g\n",
      ",\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "W\n",
      "i\n",
      "t\n",
      "h\n",
      "i\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "n\n",
      "e\n",
      " \n",
      "o\n",
      "w\n",
      "n\n",
      " \n",
      "b\n",
      "u\n"
     ]
    }
   ],
   "source": [
    "# Create Training Sequences\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
    "\n",
    "for i in char_dataset.take(500):\n",
    "     print(ind_to_char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c23d855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_len+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12f59bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq_targets(seq):\n",
    "    input_txt = seq[:-1]\n",
    "    target_txt = seq[1:]\n",
    "    return input_txt, target_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d9fe50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(create_seq_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53250319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0\n",
      "  1  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74\n",
      "  1 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45\n",
      " 63 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74\n",
      " 60  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75]\n",
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But\n",
      "\n",
      "\n",
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0  1\n",
      "  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74  1\n",
      " 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45 63\n",
      " 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74 60\n",
      "  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75  1]\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But \n"
     ]
    }
   ],
   "source": [
    "for input_txt, target_txt in  dataset.take(1):\n",
    "    print(input_txt.numpy())\n",
    "    print(''.join(ind_to_char[input_txt.numpy()]))\n",
    "    print('\\n')\n",
    "    print(target_txt.numpy())\n",
    "    # There is an extra whitespace!\n",
    "    print(''.join(ind_to_char[target_txt.numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0717c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "batch_size = 128\n",
    "\n",
    "# Buffer size to shuffle the dataset so it doesn't attempt to shuffle\n",
    "# the entire sequence in memory. Instead, it maintains a buffer in which it shuffles elements\n",
    "buffer_size = 10000\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e4548de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((128, 120), (128, 120)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d6d282",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe6adba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embed_dim = 64\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_neurons = 1026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da0934bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense,Embedding,Dropout,GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76bd2b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0aae88f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sparse_categorical_crossentropy in module tensorflow.python.keras.losses:\n",
      "\n",
      "sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sparse_categorical_crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "163d8f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_cat_loss(y_true,y_pred):\n",
    "  return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d47cf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embed_dim,batch_input_shape=[batch_size, None]))\n",
    "    model.add(GRU(rnn_neurons,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))\n",
    "    # Final Dense Layer to Predict\n",
    "    model.add(Dense(vocab_size))\n",
    "    model.compile(optimizer='adam', loss=sparse_cat_loss) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ba1e8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(\n",
    "  vocab_size = vocab_size,\n",
    "  embed_dim=embed_dim,\n",
    "  rnn_neurons=rnn_neurons,\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c6aeddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (128, None, 64)           5376      \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (128, None, 1026)         3361176   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (128, None, 84)           86268     \n",
      "=================================================================\n",
      "Total params: 3,452,820\n",
      "Trainable params: 3,452,820\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7b6144",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0d005ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 120, 84)  <=== (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "\n",
    "  # Predict off some random batch\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "\n",
    "  # Display the dimensions of the predictions\n",
    "  print(example_batch_predictions.shape, \" <=== (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c65864eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=5270, shape=(128, 120, 84), dtype=float32, numpy=\n",
       "array([[[-4.84245550e-03,  3.83646134e-03, -3.26628564e-04, ...,\n",
       "          4.16178117e-03,  1.70068129e-03,  5.59370732e-03],\n",
       "        [-4.12557647e-03,  5.98198781e-03,  7.27576716e-03, ...,\n",
       "          4.25614091e-03, -1.13632086e-04,  4.80350573e-03],\n",
       "        [ 1.87194371e-03,  1.49253255e-03,  3.21281352e-03, ...,\n",
       "          7.05804070e-03, -3.27241444e-03,  1.84733723e-03],\n",
       "        ...,\n",
       "        [-6.16176706e-03,  7.78104225e-03,  5.02144452e-03, ...,\n",
       "          5.78322820e-03,  3.42672225e-03,  1.04999011e-02],\n",
       "        [-1.02631971e-02,  1.27176112e-02,  2.91120005e-03, ...,\n",
       "          5.31574804e-03,  5.30214747e-03,  7.36269914e-03],\n",
       "        [-1.86618461e-04,  6.04301598e-03, -9.53528099e-04, ...,\n",
       "          6.95414701e-03,  7.55108544e-04,  4.05153912e-03]],\n",
       "\n",
       "       [[ 6.68125646e-03, -3.15901055e-03, -3.95030668e-03, ...,\n",
       "          8.32863525e-03,  2.94342451e-03,  5.94280753e-03],\n",
       "        [ 8.24613497e-03, -2.25267909e-03, -1.95698719e-03, ...,\n",
       "          8.33979249e-03, -2.09023268e-03,  2.38673761e-03],\n",
       "        [ 4.06389125e-03,  5.69924992e-03,  2.90279882e-03, ...,\n",
       "          9.25339758e-04, -1.83180952e-03, -3.45258764e-03],\n",
       "        ...,\n",
       "        [-6.89342665e-03,  7.11434754e-03,  4.94339829e-03, ...,\n",
       "          6.64937077e-03, -1.84964354e-03,  6.76492136e-03],\n",
       "        [ 1.09960663e-03,  2.84923124e-03,  3.32959997e-03, ...,\n",
       "          7.62253394e-03, -3.35472007e-03,  1.69937615e-03],\n",
       "        [-1.06958207e-03,  6.03394862e-03,  7.18963286e-03, ...,\n",
       "          6.37125736e-03, -2.62712082e-03,  3.29142367e-03]],\n",
       "\n",
       "       [[-6.70937169e-03,  8.59051291e-03,  1.44030200e-03, ...,\n",
       "          2.99378391e-03,  1.60904496e-03,  2.45977892e-03],\n",
       "        [-5.08000702e-03,  8.52450356e-03,  5.97199798e-03, ...,\n",
       "          4.00648639e-03, -5.35600760e-04,  5.42359613e-03],\n",
       "        [ 3.37648112e-03,  2.00856084e-06, -1.66299858e-03, ...,\n",
       "          1.11204954e-02,  1.73835445e-03,  9.82856657e-03],\n",
       "        ...,\n",
       "        [-1.44133647e-03, -4.96657012e-05, -4.45545558e-03, ...,\n",
       "         -6.54896908e-03, -7.25435419e-03, -5.31866867e-03],\n",
       "        [ 2.04803131e-04,  3.31302336e-03, -1.21608330e-02, ...,\n",
       "         -6.23884192e-03,  1.02987979e-03, -8.06236919e-03],\n",
       "        [ 4.34128661e-03,  1.05542887e-03, -7.16226408e-03, ...,\n",
       "          1.67062122e-03, -2.27849488e-03, -4.38801246e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 4.89310035e-03, -3.16694437e-04, -8.40217574e-04, ...,\n",
       "          4.28115996e-03, -2.51408806e-03, -3.87025473e-04],\n",
       "        [ 7.92122819e-03,  1.45506448e-04, -2.85614398e-03, ...,\n",
       "          6.61039585e-03, -3.70783708e-03, -5.29769459e-04],\n",
       "        [ 8.00764654e-03,  3.26772640e-03, -3.45116365e-03, ...,\n",
       "          3.98312230e-03, -3.59630911e-03,  1.27335545e-03],\n",
       "        ...,\n",
       "        [ 4.39800741e-03, -8.26971023e-04, -3.09731404e-04, ...,\n",
       "          1.99204637e-03,  5.87041024e-03,  1.90070888e-03],\n",
       "        [ 2.41804612e-03,  3.52707389e-03,  1.23026408e-03, ...,\n",
       "          4.32371441e-03,  1.38421077e-03,  5.91282547e-03],\n",
       "        [ 2.18281653e-04,  8.58185906e-03,  4.96387668e-03, ...,\n",
       "         -1.57413713e-03,  1.54621131e-03, -1.96564174e-03]],\n",
       "\n",
       "       [[ 4.89310035e-03, -3.16694437e-04, -8.40217574e-04, ...,\n",
       "          4.28115996e-03, -2.51408806e-03, -3.87025473e-04],\n",
       "        [ 7.92122819e-03,  1.45506448e-04, -2.85614398e-03, ...,\n",
       "          6.61039585e-03, -3.70783708e-03, -5.29769459e-04],\n",
       "        [ 9.83932521e-03,  8.08332989e-04, -4.69958223e-03, ...,\n",
       "          7.65514094e-03, -4.33752360e-03, -4.96335386e-04],\n",
       "        ...,\n",
       "        [ 1.29740946e-02,  2.88286782e-03, -8.10270663e-03, ...,\n",
       "          8.03040527e-03, -5.54267550e-03,  5.34130959e-05],\n",
       "        [ 1.29873203e-02,  2.89012026e-03, -8.11495632e-03, ...,\n",
       "          8.03462323e-03, -5.55679109e-03,  7.42933480e-05],\n",
       "        [ 1.29953045e-02,  2.89392821e-03, -8.12215358e-03, ...,\n",
       "          8.03815015e-03, -5.56523772e-03,  8.96090642e-05]],\n",
       "\n",
       "       [[ 1.17938616e-03,  2.55194702e-03,  1.91166310e-03, ...,\n",
       "          3.97027517e-03, -2.36975146e-03,  4.65399167e-03],\n",
       "        [-1.64980156e-05,  4.50734980e-03,  8.71170498e-03, ...,\n",
       "          8.08717869e-03, -5.89557458e-03,  9.13593732e-03],\n",
       "        [-5.38103888e-03,  6.12658495e-03,  4.97808727e-03, ...,\n",
       "          8.57495051e-03, -1.12870813e-03,  8.52627307e-03],\n",
       "        ...,\n",
       "        [-1.05037624e-02,  8.27517360e-03,  4.77749808e-03, ...,\n",
       "          1.75623421e-03,  1.06526725e-03,  5.94323035e-03],\n",
       "        [-1.41374941e-03,  4.03812481e-03,  2.69774813e-04, ...,\n",
       "          5.83495712e-03,  1.38958741e-04,  2.27267505e-03],\n",
       "        [-7.58276647e-03,  9.53622721e-03, -2.14997461e-04, ...,\n",
       "          5.90913696e-03, -3.42513551e-04,  3.65403784e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "02cdec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84a3b19f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=5276, shape=(120, 1), dtype=int64, numpy=\n",
       "array([[59],\n",
       "       [ 3],\n",
       "       [44],\n",
       "       [66],\n",
       "       [77],\n",
       "       [70],\n",
       "       [20],\n",
       "       [10],\n",
       "       [69],\n",
       "       [55],\n",
       "       [13],\n",
       "       [65],\n",
       "       [63],\n",
       "       [28],\n",
       "       [41],\n",
       "       [42],\n",
       "       [74],\n",
       "       [64],\n",
       "       [78],\n",
       "       [59],\n",
       "       [13],\n",
       "       [60],\n",
       "       [70],\n",
       "       [68],\n",
       "       [14],\n",
       "       [54],\n",
       "       [46],\n",
       "       [71],\n",
       "       [17],\n",
       "       [58],\n",
       "       [83],\n",
       "       [12],\n",
       "       [58],\n",
       "       [78],\n",
       "       [51],\n",
       "       [38],\n",
       "       [82],\n",
       "       [20],\n",
       "       [50],\n",
       "       [22],\n",
       "       [58],\n",
       "       [29],\n",
       "       [17],\n",
       "       [82],\n",
       "       [ 1],\n",
       "       [50],\n",
       "       [28],\n",
       "       [37],\n",
       "       [25],\n",
       "       [47],\n",
       "       [82],\n",
       "       [48],\n",
       "       [81],\n",
       "       [45],\n",
       "       [22],\n",
       "       [ 7],\n",
       "       [71],\n",
       "       [ 1],\n",
       "       [66],\n",
       "       [61],\n",
       "       [11],\n",
       "       [38],\n",
       "       [67],\n",
       "       [53],\n",
       "       [44],\n",
       "       [ 6],\n",
       "       [10],\n",
       "       [43],\n",
       "       [34],\n",
       "       [19],\n",
       "       [59],\n",
       "       [30],\n",
       "       [75],\n",
       "       [38],\n",
       "       [50],\n",
       "       [ 4],\n",
       "       [43],\n",
       "       [12],\n",
       "       [ 7],\n",
       "       [53],\n",
       "       [10],\n",
       "       [81],\n",
       "       [53],\n",
       "       [18],\n",
       "       [44],\n",
       "       [76],\n",
       "       [11],\n",
       "       [ 3],\n",
       "       [32],\n",
       "       [21],\n",
       "       [72],\n",
       "       [79],\n",
       "       [ 6],\n",
       "       [76],\n",
       "       [71],\n",
       "       [ 2],\n",
       "       [25],\n",
       "       [12],\n",
       "       [66],\n",
       "       [12],\n",
       "       [68],\n",
       "       [64],\n",
       "       [49],\n",
       "       [81],\n",
       "       [73],\n",
       "       [65],\n",
       "       [80],\n",
       "       [71],\n",
       "       [34],\n",
       "       [37],\n",
       "       [69],\n",
       "       [ 1],\n",
       "       [59],\n",
       "       [24],\n",
       "       [59],\n",
       "       [29],\n",
       "       [62],\n",
       "       [72],\n",
       "       [66],\n",
       "       [25]], dtype=int64)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e813a1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat to not be a lists of lists\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "246db5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the input seq: \n",
      "\n",
      "nd him  \n",
      "    The greatness he has got. I hourly learn\n",
      "    A doctrine of obedience, and would gladly\n",
      "    Look him i' th' \n",
      "\n",
      "\n",
      "Next Char Predictions: \n",
      "\n",
      "d\"Skvo9.n`2jhCPQsiwd2eom3_Up6c}1cwZM|9Y;cD6| YCL?V|WzT;)p kf0Ml]S(.RI8dEtMY&R1)].z]7Su0\"G:qx(up!?1k1miXzrjypILn d>dDgqk?\n"
     ]
    }
   ],
   "source": [
    "print(\"Given the input seq: \\n\")\n",
    "print(\"\".join(ind_to_char[input_example_batch[0]]))\n",
    "print('\\n')\n",
    "print(\"Next Char Predictions: \\n\")\n",
    "print(\"\".join(ind_to_char[sampled_indices ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747171cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "      8/Unknown - 45s 6s/step - loss: 3.0929"
     ]
    }
   ],
   "source": [
    "model.fit(dataset,epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a839c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('shakespeare_gen.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c7d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d1a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size=1)\n",
    "\n",
    "model.load_weights('shakespeare_gen.h5')\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8700c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba45297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_seed,gen_size=100,temp=1.0):\n",
    "  '''\n",
    "  model: Trained Model to Generate Text\n",
    "  start_seed: Intial Seed text in string form\n",
    "  gen_size: Number of characters to generate\n",
    "\n",
    "  Basic idea behind this function is to take in some seed text, format it so\n",
    "  that it is in the correct shape for our network, then loop the sequence as\n",
    "  we keep adding our own predicted characters. Similar to our work in the RNN\n",
    "  time series problems.\n",
    "  '''\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = gen_size\n",
    "\n",
    "  # Vecotrizing starting seed text\n",
    "  input_eval = [char_to_ind[s] for s in start_seed]\n",
    "\n",
    "  # Expand to match batch format shape\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty list to hold resulting generated text\n",
    "  text_generated = []\n",
    "\n",
    "  # Temperature effects randomness in our resulting text\n",
    "  # The term is derived from entropy/thermodynamics.\n",
    "  # The temperature is used to effect probability of next characters.\n",
    "  # Higher probability == lesss surprising/ more expected\n",
    "  # Lower temperature == more surprising / less expected\n",
    " \n",
    "  temperature = temp\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "\n",
    "  for i in range(num_generate):\n",
    "\n",
    "      # Generate Predictions\n",
    "      predictions = model(input_eval)\n",
    "\n",
    "      # Remove the batch shape dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # Use a cateogircal disitribution to select the next character\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # Pass the predicted charracter for the next input\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      # Transform back to character letter\n",
    "      text_generated.append(ind_to_char[predicted_id])\n",
    "\n",
    "  return (start_seed + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa97cf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(model,\"flower\",gen_size=1000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
